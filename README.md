
# ğŸš¢ Comparative Analysis of Machine Learning Models on the Titanic Classification Dataset ğŸ¤–

This project applies multiple machine learning algorithms to the famous Titanic dataset ğŸ›³ï¸ in order to predict passenger survival ğŸ¯. The dataset contains demographic and travel-related features such as age ğŸ‘¶ğŸ‘´, sex ğŸš¹ğŸšº, fare ğŸ’µ, passenger class ğŸŸï¸, and embarkation point âš“.

The study evaluates the performance of several classification models, including:
ğŸ”¹ K-Nearest Neighbors (KNN)
ğŸ”¹ Support Vector Classifier (SVC)
ğŸ”¹ Random Forest ğŸŒ²
ğŸ”¹ Decision Tree ğŸŒ³
ğŸ”¹ Logistic Regression ğŸ“ˆ

Each model was trained and tested on the dataset, and their accuracy scores ğŸ“Š were compared to determine their effectiveness in survival prediction.

Through this comparative analysis, the project highlights the strengths ğŸ’ª and limitations âš ï¸ of different classification techniques when applied to a real-world dataset. The results provide insights into which models are better suited for structured datasets with mixed categorical and numerical features ğŸ”¢, such as the Titanic dataset.

## Introduction
The Titanic â€“ Machine Learning from Disaster dataset ğŸš¢ is one of the most iconic beginner-friendly datasets for classification tasks ğŸ¤–. This project focuses on building predictive models to determine whether a passenger survived the Titanic tragedy ğŸ’¡ based on features such as age ğŸ‘¶ğŸ‘´, sex ğŸš¹ğŸšº, passenger class ğŸŸï¸, fare ğŸ’µ, and embarkation point âš“.

The workflow includes:
ğŸ”¹ Data Cleaning ğŸ§¹
ğŸ”¹ Exploratory Data Analysis (EDA) ğŸ”
ğŸ”¹ Feature Engineering ğŸ› ï¸
ğŸ”¹ Model Training & Evaluation ğŸ“Š

Several machine learning models were implemented, including:
âœ¨ Logistic Regression ğŸ“ˆ
âœ¨ K-Nearest Neighbors (KNN) ğŸ‘¥
âœ¨ Support Vector Classifier (SVC) ğŸ“
âœ¨ Decision Tree ğŸŒ³
âœ¨ Random Forest ğŸŒ²

The main objectives of this project are:
1ï¸âƒ£ Practical Learning ğŸ¯ â€“ to gain hands-on experience with the machine learning pipeline, from preprocessing to evaluation.
2ï¸âƒ£ Comparative Analysis âš–ï¸ â€“ to compare multiple classification algorithms and identify which performs best on the Titanic dataset.

This hands-on exercise serves as a strong foundation for understanding supervised learning concepts ğŸ§ , model selection ğŸ”„, and the importance of preprocessing ğŸ§© in real-world datasets.

## Titanic Dataset â€“ Feature Explanation

1. **PassengerId ğŸ”¢**

   * Unique identifier for each passenger.
   * Not useful for prediction (just an index).

2. **Survived âš°ï¸â¡ï¸ğŸ™‚**

   * Target variable (0 = did not survive, 1 = survived).
   * What your models are trying to predict.

3. **Pclass ğŸŸï¸**

   * Passengerâ€™s ticket class:

     * 1 = First Class (luxury ğŸ¥‚)
     * 2 = Second Class (middle ğŸ’º)
     * 3 = Third Class (economy ğŸš¶)
   * A proxy for socio-economic status.

4. **Name ğŸ·ï¸**

   * Passengerâ€™s full name.
   * Can be used to extract **titles** (Mr, Mrs, Miss, etc.) which often help prediction.

5. **Sex ğŸš¹ğŸšº**

   * Gender of the passenger.
   * Very important feature (women had higher survival rates).

6. **Age ğŸ‘¶ğŸ‘¦ğŸ‘©ğŸ‘´**

   * Passengerâ€™s age in years.
   * Some values are missing (\~177 NaNs).
   * Survival rates often varied by age group (children had priority).

7. **SibSp ğŸ‘¨â€ğŸ‘©â€ğŸ‘§**

   * Number of siblings/spouses aboard.
   * Shows family size onboard.

8. **Parch ğŸ‘¶ğŸ‘µ**

   * Number of parents/children aboard.
   * Another indicator of family size.

9. **Ticket ğŸ«**

   * Ticket number (categorical).
   * Not very predictive directly, but can sometimes encode group/family info.

10. **Fare ğŸ’µ**

* Price paid for the ticket.
* Higher fares often linked to higher class (and higher survival chances).

11. **Cabin ğŸšª**

* Cabin number.
* Mostly missing (only \~204 filled).
* Can sometimes hint at passenger class and location on the ship.

12. **Embarked âš“**

* Port of Embarkation:

  * C = Cherbourg ğŸ‡«ğŸ‡·
  * Q = Queenstown ğŸ‡®ğŸ‡ª
  * S = Southampton ğŸ´
* Has 2 missing values.

---

âœ… **Summary:**

* **Categorical features**: Sex, Embarked, Pclass, Name (derived titles), Ticket, Cabin
* **Numerical features**: Age, SibSp, Parch, Fare
* **Target variable**: Survived

---

## Results
The performance of different classification models was evaluated on the Titanic dataset ğŸš¢ to predict passenger survival âš°ï¸â¡ï¸ğŸ™‚. Each model was trained and tested after preprocessing the data, and their accuracy scores ğŸ¯ were compared:

ğŸ“ˆ Logistic Regression â†’ ~80% accuracy

ğŸŒ³ Decision Tree â†’ ~77% accuracy

ğŸŒ² Random Forest â†’ ~82% accuracy

ğŸ‘¥ K-Nearest Neighbors (KNN) â†’ ~78% accuracy

ğŸ“ Support Vector Classifier (SVC) â†’ ~78% accuracy

âœ… Key Insight: Ensemble models like Random Forest ğŸŒ² achieved the highest accuracy, while Logistic Regression ğŸ“ˆ also performed strongly as a baseline model. Simpler models like Decision Trees ğŸŒ³ and KNN ğŸ‘¥ gave competitive but slightly lower results.

This comparison highlights the importance of trying multiple algorithms âš–ï¸ and selecting the best-performing one for the problem at hand.
## Future Works/Improvements
This project already includes strong feature engineering (e.g., extracted passenger titles ğŸ·ï¸, created family size ğŸ‘¨â€ğŸ‘©â€ğŸ‘§) and hyperparameter tuning âš™ï¸. Possible next steps to expand the project include:

âœ¨ **More Advanced Models** ğŸ¤–

ğŸ”¹Implement gradient boosting models like XGBoost âš¡, LightGBM ğŸŒŸ, or CatBoost ğŸˆ

ğŸ”¹Compare results with deep learning models (TensorFlow / PyTorch) ğŸ§ 

âœ¨ **Cross-Validation & Robust Evaluation** ğŸ”

ğŸ”¹Extend beyond a simple train-test split to k-fold cross-validation for more reliable accuracy.

âœ¨ **Better Handling of Missing Data** ğŸ§©

ğŸ”¹Try advanced imputation methods (e.g., KNN imputer, iterative imputer) instead of simple fill techniques.

âœ¨ **Model Interpretability** ğŸ”

ğŸ”¹Use SHAP or LIME to explain predictions and identify the most influential features.

âœ¨ **Deployment** ğŸš€

ğŸ”¹Create a Streamlit/Dash web app for interactive survival prediction ğŸŒ Or develop a REST API with FastAPI/Flask so the model can be integrated into other systems.
## Acknowledgements
This project is heavily inspired ğŸ™ by the work of [Berkay DoÄŸan](https://www.linkedin.com/in/berkaydogan-/)
 ğŸ‘¨â€ğŸ’». His [kaggle Profile](https://www.kaggle.com/berkaydogan1)
  provided the foundation for much of the code used here. While several parts were adapted ğŸ”§, the overall structure is almost a replica of his implementation.

The main ambition behind this project was to learn coding practices ğŸ’» and develop intuition for machine learning workflows ğŸ¤–. I believe that studying ğŸ“š and replicating well-written code is a valuable step in building a deeper understanding ğŸ’¡ of the concepts and techniques behind data science projects ğŸ”¬.

If you want to know more this is his [Github-Profile](https://github.com/berkaydgn).
 ğŸ™.
